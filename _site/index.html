<!DOCTYPE html>

<html lang="en">
    <head>

        <!-- Metadata -->
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
        <meta name="description" content="website"/>
        <meta name="author" content="khang nguyen"/>
        <title>khang nguyen | bscs'24 @ uta</title>
        <link rel="icon" type="image/x-icon" href="assets/img/mandu_icon.png"/>
        
        <!-- Font Awesome icons -->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js"></script>
		
        <!-- Google fonts-->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">        
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css"/>
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.theme.default.min.css">

        <!-- Core theme CSS -->
        <link href="styles/styles.css" rel="stylesheet"/>

    </head>

    <body class="light-theme">

        <!-- Moving particles -->
        <canvas id="canvas"></canvas>

        <!-- Progress bar on top -->
        <div class="progress-bar-container">
            <div class="progress-bar" id="progressBar"></div>
        </div>

        <!-- Back to top button -->
        <a id="back-to-top-button"></a>

        <!-- Toggle dark/light theme button -->
        <button class="toggle-theme-button" onclick="toggleTheme()">‚òÄÔ∏è</button>
        
        <!-- Assitant icon saying about theme changes -->
        <div class="popup-icon-container" id="popupIconContainer" draggable="true">
            <div class="icon"><img src="assets/img/mandu_icon.png" width="65" height="65"></div>
            <div class="speech-balloon"></div>
        </div>
        
        <!-- Dismissal area for assistant icon -->
        <div class="dismissal-area" id="dismissalArea">&#10006;</div>

        <!-- Content -->
        <div class="container mt-5">
            <!-- About section -->

<div class="row mb-4">
    <div class="col-lg-2 col-md-4">
        <div class="ring-container">
            <div class="ring">
                <div class="hollow-ring">
                    <img class="profile-image" src="assets/img/new_pic.jpg" alt="khang nguyen" />
                     
                        <div class="emoji-indicator">
                            üçÄ 
                            <span class="hover-text"> four-leaf clover </span> 
                        </div> 
                    
                </div>
            </div>
        </div>
        <hr />
        <div class="social-icons">	
             <a class="social-icon" href="https://scholar.google.com/citations?user=Z6_5ZTEAAAAJ" target="_blank" rel="noopener" title="Google Scholar"><i class="fa fa-graduation-cap" style="font-size: 35px; color: #4285f4"></i></a> 
             <a class="social-icon" href="https://www.researchgate.net/profile/Khang-Nguyen-133" target="_blank" rel="noopener" title="ResearchGate"><i class="fab fa-researchgate" style="font-size: 35px; color: #00D0BB"></i></a> 
             <a class="social-icon" href="https://github.com/mkhangg" target="_blank" rel="noopener" title="GitHub"><i class="fab fa-github" style="font-size: 35px; color: #171515"></i></a> 
             <a class="social-icon" href="https://www.youtube.com/@_m.khangg_" target="_blank" rel="noopener" title="YouTube"><i class="fab fa-youtube" style="font-size: 35px; color: #FF0000"></i></a> 
             <a class="social-icon" href="assets/doc/Resume_KhangNguyen.pdf" target="_blank" rel="noopener" title="Resume"><i class="fas fa-file-alt" style="font-size: 35px; color: #bd5d38"></i></a> 
        </div>
        <p></p>
    </div>

    <div class="col-lg-10 col-md-8">
        <h2>khang nguyen <span class="text-primary">/k ∞√¶≈ã/</span></h2>
        <p></p>
        I am currently a fourth-year Computer Science undergraduate at the <a href="https://www.uta.edu/academics/schools-colleges/engineering/academics/departments/cse" target="_blank" rel="noopener">University of Texas at Arlington</a> and a research assistant at the <hightlight>Learning and Adaptive Robotics Lab</hightlight>, where I am advised by <a href="https://www.uta.edu/academics/faculty/profile?username=huber" target="_blank" rel="noopener">Dr. Manfred Huber</a>. (‚óï‚ñø‚óï)
        <p></p>
        I grew up in <a href="https://en.wikipedia.org/wiki/Saigon" target="_blank" rel="noopener">Saigon, Vietnam</a>, and was fortunate to spend my most memorable time at the <a href="https://en.wikipedia.org/wiki/VNU-HCM_High_School_for_the_Gifted" target="_blank" rel="noopener">VNU-HCM High School for the Gifted</a> (informatics program of 2020) and previously <a href="https://en.wikipedia.org/wiki/Tr%E1%BA%A7n_%C4%90%E1%BA%A1i_Ngh%C4%A9a_High_School_for_the_Gifted" target="_blank" rel="noopener">TƒêN Secondary School for the Gifted</a> (mathematics program of 2017) with very good friends - too many to name them here, where we played football (soccer) on concrete fields together afterschool.      
        <p></p>
        <code>&gt;&gt; I do research in robotics, mainly focusing on applying <b>cognitive architectures to robotic grasping</b> to enable robots with a closely similar human grasping capability. Just imagine the robot enters the kitchen, finds a banana, grasps without crushing it, and brings it back to you.</code>
        <!-- <p></p> -->
    </div>
</div>

<!-- Updates section -->

<hr />

<div class="row" id="updates">
    <div class="col">
        <h2 clss="mb-5">üìç updates</h2>
        <p></p>
        <div class="owl-carousel owl-theme">
            
                <div class="news-card">
    <img src="assets/img/updates_gan/city_boy.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I will attend IROS 2023 in <hightlight>Detroit, Michigan</hightlight>.</div>
    <div class="news-time">October 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/travel_boy.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I will present at ISR 2023 in <hightlight>Stuttgart, Germany</hightlight>.</div>
    <div class="news-time">September 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/holding_can_robot.jpg" class="w-full rounded-lg" />
    <div class="news-desc">The <hightlight>deformable object classification</hightlight> paper is accepted to ISR 2023!</div>
    <div class="news-time">June 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/calibrating_robot.jpg" class="w-full rounded-lg" />
    <div class="news-desc">The <hightlight>multiplanar self-calibration</hightlight> paper is accepted to IROS 2023!</div>
    <div class="news-time">June 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/fireworks_robot.jpg" class="w-full rounded-lg" />
    <div class="news-desc">My <hightlight>thesis proposal</hightlight> is approved by the Honors College!</div>
    <div class="news-time">June 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/reading_boy_2.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I am invited to review papers at <hightlight>CASE 2023</hightlight>.</div>
    <div class="news-time">April 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/happy_robot.jpg" class="w-full rounded-lg" />
    <div class="news-desc">The <hightlight>PerFC</hightlight> paper is accepted to FLAIRS-36!</div>
    <div class="news-time">March 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/reading_boy.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I am invited to review papers at <hightlight>UR 2023</hightlight>.</div>
    <div class="news-time">February 2023</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/spider_bot.jpg" class="w-full rounded-lg" />
    <div class="news-desc"><hightlight>Spidey</hightlight> won sponsorship prizes at HackMIT 2022!</div>
    <div class="news-time">October 2022</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/boy_robot.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I joined the <hightlight>Learning and Adaptive Robotics (LEARN) Lab</hightlight> at UTA.</div>
    <div class="news-time">August 2022</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/happy_tree.jpg" class="w-full rounded-lg" />
    <div class="news-desc">The <hightlight>IoTree</hightlight> paper is accepted to MobiCom 2022!</div>
    <div class="news-time">June 2022</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/farm_bot.jpg" class="w-full rounded-lg" />
    <div class="news-desc"><hightlight>iPlanter</hightlight> won prizes at GaTech RoboTech Hackathon 2022!</div>
    <div class="news-time">April 2022</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/boy_systems.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I joined the <hightlight>Wireless and Sensor Systems Lab (WSSL)</hightlight> at UTA.</div>
    <div class="news-time">August 2021</div>
</div>
            
                <div class="news-card">
    <img src="assets/img/updates_gan/boy_school.jpg" class="w-full rounded-lg" />
    <div class="news-desc">I start my undergraduate at the <hightlight>University of Texas at Arlington (UTA)</hightlight>.</div>
    <div class="news-time">August 2020</div>
</div>
            
        </div>
        <p></p>
    </div>
</div>

<!-- Research section -->

<hr />

<div class="row" id="research">
    <div class="col">
        <h2 clss="mb-5">üìç publications</h2>
        <p></p>
        <div id="filters">
            <button class="filter-button active" data-filter="*">all</button>
            
                <button class="filter-button" data-filter="perception manipulation">perception + manipulation</button>
            
                <button class="filter-button" data-filter="framework">framework</button>
            
        </div>
        <p></p>
        <div id="projects" class="isotope">
            
                <div class="project" data-filter="perception manipulation">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_scene_perception.gif" />
        </div>
        <div class="col-sm-8">
            
            <b>Real-Time 3D Semantic Scene Perception for Egocentric Robots with Binocular Vision</b>
            <br />
            <i><a href="https://2024.ieee-icra.org/" target="_blank" rel="noopener">ICRA 2024 (Yokohama, Japan)</a></i> 
             <b style="color: #e74d3c">[Under Review]</b> 
            <br />
            <u>Khang Nguyen</u>, Tuan Dang, Manfred Huber.
            <br />
             <a href="" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/mkhangg/semantic_scene_perception" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/-dho7l_r56U" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            Perceiving a three-dimensional (3D) scene with multiple objects while moving indoors is essential for vision-based mobile cobots, especially for enhancing their manipulation tasks. In this work, we present an end-to-end pipeline with instance segmentation, feature matching, and point-set registration for egocentric robots with binocular vision, and demonstrate the robot's
            <span class="collapse" id="more_scene_perception">
                grasping capability through the proposed pipeline. First, we design an RGB image-based segmentation approach for single-view 3D semantic scene segmentation, leveraging common object classes in 2D datasets to encapsulate 3D points into point clouds of object instances through corresponding depth maps. Next, 3D correspondences of two consecutive segmented point clouds are extracted based on matched keypoints between objects of interest in RGB images from the prior step. In addition, to be aware of spatial changes in 3D feature distribution, we also weigh each 3D point pair based on the estimated distribution using kernel density estimation (KDE), which subsequently gives robustness with less central correspondences while solving for rigid transformations between point clouds. Finally, we test our proposed pipeline on the 7-DOF dual-arm Baxter robot with a mounted Intel RealSense D435i RGB-D camera. The result shows that our robot can segment objects of interest, register multiple views while moving, and grasp the target object.
            </span> 
            <span> <a href="#more_scene_perception" data-toggle="collapse" onclick="toggleText(this)" id="link-more_scene_perception">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="perception manipulation">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_online.gif" />
        </div>
        <div class="col-sm-8">
            
            <b>Online 3D Deformable Object Classification for Mobile Cobot Manipulation</b>
            <br />
            <i><a href="https://www.isr-robotics.org/isr" target="_blank" rel="noopener">ISR Europe 2023 (Stuttgart, Baden-Wurttemberg, Germany)</a></i> 
            
            <br />
            <u>Khang Nguyen</u>, Tuan Dang, Manfred Huber.
            <br />
             <a href="https://www.researchgate.net/profile/Khang-Nguyen-133/publication/374371098_Online_3D_Deformable_Object_Classification_for_Mobile_Cobot_Manipulation/links/651a2cfa1e2386049def3947/Online-3D-Deformable-Object-Classification-for-Mobile-Cobot-Manipulation.pdf" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/mkhangg/deformable_cobot" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/qkgi3T6xYzI" target="_blank" rel="noopener">[DEMO]</a> 
             | <a href="https://mkhangg.com/assets/slides/isr23_slides.pdf" target="_blank" rel="noopener">[SLIDES]</a> 
             | <a href="https://youtu.be/ATzyXtLAK6E" target="_blank" rel="noopener">[TALK]</a> 
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            Vision-based object manipulation in assistive mobile cobots essentially relies on classifying the target objects based on their 3D shapes and features, whether they are deformed or not. In this work, we present an auto-generated dataset of deformed objects specific for assistive mobile cobot manipulation using an intuitive Laplacian-based mesh deformation procedure. We
            <span class="collapse" id="more_online">
                first determine the graspable region of the robot hand on the given object's mesh. Then, we uniformly sample handle points within the graspable region and perform deformation with multiple handle points based on the robot gripper configuration. In each deformation, we identify the orientation of handle points and prevent self-intersection to guarantee the object's physical meaning when multiple handle points are simultaneously applied to the mesh at different deformation intensities. We also introduce a lightweight neural network for 3D deformable object classification. Finally, we test our generated dataset on the Baxter robot with two 7-DOF arms, an integrated RGB-D camera, and a 3D deformable object classifier. The result shows that the robot is able to classify real-world deformed objects from point clouds captured at multiple views by the RGB-D camera.
            </span> 
            <span> <a href="#more_online" data-toggle="collapse" onclick="toggleText(this)" id="link-more_online">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="perception manipulation">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_multiplanar.gif" />
        </div>
        <div class="col-sm-8">
            
            <b>Multiplanar Self-Calibration for Mobile Cobot 3D Object Manipulation using 2D Detectors and Depth Estimation</b>
            <br />
            <i><a href="https://ieee-iros.org/" target="_blank" rel="noopener">IROS 2023 (Detroit, MI, U.S.)</a></i> 
            
            <br />
            Tuan Dang, <u>Khang Nguyen</u>, Manfred Huber.
            <br />
             <a href="https://www.researchgate.net/profile/Tuan-Dang-25/publication/374503139_Multiplanar_Self-Calibration_for_Mobile_Cobot_3D_Object_Manipulation_using_2D_Detectors_and_Depth_Estimation/links/6520351ab0df2f20a2163da6/Multiplanar-Self-Calibration-for-Mobile-Cobot-3D-Object-Manipulation-using-2D-Detectors-and-Depth-Estimation.pdf" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/calib_cobot" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/KrDJ22rvOAo" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            Calibration is the first and foremost step in dealing with sensor displacement errors that can appear during extended operation and off-time periods to enable robot object manipulation with precision. In this paper, we present a novel multiplanar self-calibration between the
            <span class="collapse" id="more_multiplanar">
                camera system and the robot's end-effector for 3D object manipulation. Our approach first takes the robot end-effector as ground truth to calibrate the camera‚Äôs position and orientation while the robot arm moves the object in multiple planes in 3D space, and a 2D state-of-the-art vision detector identifies the object‚Äôs center in the image coordinates system. The transformation between world coordinates and image coordinates is then computed using 2D pixels from the detector and 3D known points obtained by robot kinematics. Next, an integrated stereo-vision system estimates the distance between the camera and the object, resulting in 3D object localization. We test our proposed method on the Baxter robot with two 7-DOF arms and a 2D detector that can run in real time on an onboard GPU. After self-calibrating, our robot can localize objects in 3D using an RGB camera and depth image.
            </span> 
            <span> <a href="#more_multiplanar" data-toggle="collapse" onclick="toggleText(this)" id="link-more_multiplanar">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="framework">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_extperfc.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>ExtPerFC</i>:</b> 
            <b>An Efficient 2D &amp; 3D Perception Software-Hardware Framework for Mobile Cobot</b>
            <br />
            <i><a href="" target="_blank" rel="noopener">arXiv (06/08/2023)</a></i> 
            
            <br />
            Tuan Dang, <u>Khang Nguyen</u>, Manfred Huber.
            <br />
             <a href="https://arxiv.org/pdf/2306.04853.pdf" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/perception_framework" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/q4oz9Rixbzs" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            As the reliability of the robot's perception correlates with the number of integrated sensing modalities to tackle uncertainty, a practical solution to manage these sensors from different computers, operate them simultaneously, and maintain their real-time performance on the existing robotic system with minimal effort is needed. In this work, we present an end-to-end software-hardware
            <span class="collapse" id="more_extperfc">
                framework, namely <i>ExtPerFC</i>, that supports both conventional hardware and software components and integrates machine learning object detectors without requiring an additional dedicated graphic processor unit (GPU). We first design our framework to achieve real-time performance on the existing robotic system, guarantee configuration optimization, and concentrate on code reusability. We then mathematically model and utilize our transfer learning strategies for 2D object detection and fuse them into depth images for 3D depth estimation. Lastly, we systematically test the proposed framework on the Baxter robot with two 7-DOF arms, a four-wheel mobility base, and an Intel RealSense D435i RGB-D camera. The results show that the robot achieves real-time performance while executing other tasks (<i>e.g.</i>, map building, localization, navigation, object detection, arm moving, and grasping) simultaneously with available hardware like Intel onboard CPUs/GPUs on distributed computers. Also, to comprehensively control, program, and monitor the robot system, we design and introduce an end-user application.
            </span> 
            <span> <a href="#more_extperfc" data-toggle="collapse" onclick="toggleText(this)" id="link-more_extperfc">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="framework">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_perfc.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>PerFC</i>:</b> 
            <b>An Efficient 2D and 3D Perception Software-Hardware Framework for Mobile Cobot</b>
            <br />
            <i><a href="https://www.flairs-36.info/home" target="_blank" rel="noopener">FLAIRS-36 (Clearwater Beach, FL, U.S.)</a></i> 
            
            <br />
            Tuan Dang, <u>Khang Nguyen</u>, Manfred Huber.
            <br />
             <a href="https://journals.flvc.org/FLAIRS/article/view/133316/137627" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/perception_framework" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/q4oz9Rixbzs" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            In this work, we present an end-to-end software-hardware framework that supports both conventional hardware and software components and integrates machine learning object detectors without requiring an additional dedicated graphic processor unit (GPU). We design our framework to achieve real-time performance on the robot system, guarantee such performance on
            <span class="collapse" id="more_perfc">
                multiple computing devices, and concentrate on code reusability. We then utilize transfer learning strategies for 2D object detection and fuse them into depth images for 3D depth estimation. Lastly, we test the proposed framework on the Baxter robot with two 7-DOF arms and a four-wheel mobility base. The results show that the robot achieves real-time performance while executing other tasks (map building, localization, navigation, object detection, arm moving, and grasping) with available hardware like Intel onboard GPUs on distributed computers. Also, to comprehensively control, program, and monitor the robot system, we  design and introduce an end-user application.
            </span> 
            <span> <a href="#more_perfc" data-toggle="collapse" onclick="toggleText(this)" id="link-more_perfc">... See More</a></span>
        </div>                       
    </div>
</div>

            
                <div class="project" data-filter="">
    <div class="row mb-4">
        <div class="col-sm-4">
            <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_iotree.gif" />
        </div>
        <div class="col-sm-8">
             <b><i>IoTree</i>:</b> 
            <b>A Battery-free Wearable System with Biocompatible Sensors for Continuous Tree Health Monitoring</b>
            <br />
            <i><a href="https://www.sigmobile.org/mobicom/2022/" target="_blank" rel="noopener">MobiCom 2022 (Sydney, NSW, Australia)</a></i> 
            
            <br />
            Tuan Dang, Trung Tran, <u>Khang Nguyen</u>, Tien Pham, Nhat Pham, Tam Vu, Phuc Nguyen.
            <br />
             <a href="https://dl.acm.org/doi/pdf/10.1145/3495243.3567652" target="_blank" rel="noopener">[PDF]</a> 
             | <a href="https://github.com/tuantdang/iotree" target="_blank" rel="noopener">[CODE]</a> 
             | <a href="https://youtu.be/8DUfOcuPwIk" target="_blank" rel="noopener">[DEMO]</a> 
            
            
            <br />
            <u><b><i>Abstract</i></b>:</u> 
            In this paper, we present a low-maintenance, wind-powered, batteryfree, biocompatible, tree wearable, and intelligent sensing system, namely <i>IoTree</i>, to monitor water and nutrient levels inside a living tree. <i>IoTree</i> system includes tiny-size, biocompatible, and implantable sensors that
            <span class="collapse" id="more_iotree">
                continuously measure the impedance variations inside the living tree‚Äôs xylem, where water and nutrients are transported from the root to the upper parts. The collected data are then compressed and transmitted to a base station located at up to 1.8 kilometers (approximately 1.1 miles) away. The entire <i>IoTree</i> system is powered by wind energy and controlled by an adaptive computing technique called block-based intermittent computing, ensuring the forward progress and data consistency under intermittent power and allowing the firmware to execute with the most optimal memory and energy usage. We prototype <i>IoTree</i> that opportunistically performs sensing, data compression, and long-range communication tasks without batteries. During in-lab experiments, <i>IoTree</i> also obtains the accuracy of 91.08% and 90.51% in measuring 10 levels of nutrients, NH<sub>3</sub> and K<sub>2</sub>O, respectively. While tested with Burkwood Viburnum and White Bird trees in the indoor environment, <i>IoTree</i> data strongly correlated with multiple watering and fertilizing events. We also deployed <i>IoTree</i> on a grapevine farm for 30 days, and the system is able to provide sufficient measurements every day.
            </span> 
            <span> <a href="#more_iotree" data-toggle="collapse" onclick="toggleText(this)" id="link-more_iotree">... See More</a></span>
        </div>                       
    </div>
</div>

            
        </div>
        <p></p>
    </div>
</div>

<!-- Outreach section -->

<hr />

<div class="row" id="outreach">
    <div class="col">
        <h2 clss="mb-5">üìç outreach activities</h2>
        <p></p>
        
            <div class="row mb-4">
    <div class="col-sm-4">
        <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_spidey.gif" />
    </div>
    <div class="col-sm-8">
         <b><i>Spidey</i>:</b> 
        <b>An Autonomous Spatial Voice Localization Crawling Robot</b>
        <br />
        <i><a href="https://hackmit.org/" target="_blank" rel="noopener">HackMIT 2022 (Boston, MA, U.S.)</a></i>
        <br />
        <u>Khang Nguyen</u>.
        <br />
         <a href="https://spectacle.hackmit.org/project/185" target="_blank" rel="noopener">[POST]</a> 
         | <a href="https://github.com/mkhangg/hackmit22" target="_blank" rel="noopener">[CODE]</a> 
         | <a href="https://youtu.be/m1g6fkH6Zvg" target="_blank" rel="noopener">[DEMO]</a> 
        <br />
        <u><b><i>Description</i></b>:</u> We present an autonomous spatial voice localization crawling robot demonstrating the potential of assistive technology for people with visual impairment to ask for help whenever they are in a spatial area without physical assistance.
        <br />
        <u><b><i>Prize</i></b>:</u> Won Sponsorship Award for Assistive Technologies over 198 competing teams.
    </div>
</div>

        
            <div class="row mb-4">
    <div class="col-sm-4">
        <img width="100%" height="auto" class="w-full rounded-lg" src="assets/img/demo_iplanter.gif" />
    </div>
    <div class="col-sm-8">
         <b><i>iPlanter</i>:</b> 
        <b>An Autonomous Ground Monitoring and Tree Planting Robot</b>
        <br />
        <i><a href="https://robotech2022.devpost.com/" target="_blank" rel="noopener">GT IEEE RoboTech 2022 (Atlanta, GA, U.S.)</a></i>
        <br />
        <u>Khang Nguyen</u>, Muhtasim Mahfuz, Vincent Kipchoge, Johnwon Hyeon.
        <br />
         <a href="https://devpost.com/software/tree-planting-robot" target="_blank" rel="noopener">[POST]</a> 
         | <a href="https://github.com/mkhangg/robotech22" target="_blank" rel="noopener">[CODE]</a> 
         | <a href="https://youtu.be/GZ0oAX-lLSM" target="_blank" rel="noopener">[DEMO]</a> 
        <br />
        <u><b><i>Description</i></b>:</u> Our tree planting robot demonstrates the an on-farm surveying robot that autonomously determines soil quality, plants seeds, and collects on-ground images.
        <br />
        <u><b><i>Prize</i></b>:</u> Won 2nd place in Body Track, 3rd place in Electrical Track, and Top 8 Prizes over 47 competing teams (approximately 160 participants).
    </div>
</div>

        
        <p></p>
    </div>
</div>

<!-- Gallery section -->

<hr />

<div class="row" id="gallery">
    <div class="col">
        <h2 clss="mb-5">üìç gallery</h2>
        <p></p>
        not all about robots, but also some happy moments of my life.
        <p></p>
        
            <div class="gallery">
    <img src="assets/memo/iros23.jpg" alt="me and tuan also met new friends at iros" width="800" height="600" />
    <div class="desc">me and tuan also met new friends at iros (2023)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/snow_football.jpg" alt="first time playing football on snow" width="800" height="600" />
    <div class="desc">first time playing football on snow (2022)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/aquarium.jpg" alt="i saw school of fish at georgia aquarium" width="800" height="600" />
    <div class="desc">i saw school of fish at georgia aquarium (2021)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/ki_yeu.jpg" alt="these crazy boys from ptnk i1720" width="800" height="600" />
    <div class="desc">these crazy boys from ptnk i1720 (2020)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/harvard.jpg" alt="wolfram summer research program fieldtrip" width="800" height="600" />
    <div class="desc">wolfram summer research program fieldtrip (2019)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/family.jpg" alt="my sister started her mba" width="800" height="600" />
    <div class="desc">my sister started her mba (2018)</div>
</div>

        
            <div class="gallery">
    <img src="assets/memo/doituyentoan.jpg" alt="my boys from tƒën maths team" width="800" height="600" />
    <div class="desc">my boys from tƒën maths team (2017)</div>
</div>

        
        <p></p>
    </div>
</div>

<!-- Footer section -->

<div>‚Äé</div>
<hr />

<footer class="pt-2 my-md-2 pt-md-">
    <div class="row justify-content-center">
        <div class="col-7 col-md text-left align-self-center">
            <p class="h6">¬© khang nguyen, <span id="currentYear"></span></p>
            <a href="https://github.com/mkhangg/academic-website" target="_blank" rel="noopener"><b>&gt; web source @github</b></a>
        </div>
        <div class="col col-md text-right">
            
                <img class="mr+4" src="assets/img/uta_cse_logo.png" data-canonical-src="assets/img/uta_cse_logo.png" alt="UTA CSE" width="60" />
            
                <img class="mr+4" src="assets/img/uta_logo.png" data-canonical-src="assets/img/uta_logo.png" alt="UTA" width="60" />
            
        </div>
    </div>
    <p></p>
</footer>


        </div>

        <!-- Bootstrap core JS-->
        <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>

        <!-- Isotope JS -->
        <script src="https://cdn.jsdelivr.net/npm/isotope-layout@3.0.2/dist/isotope.pkgd.min.js"></script>

        <!-- OwlCarousel2 JS -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js"></script>
        
		<!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>

		<!-- Core theme JS-->
        <script src="js/scripts.js"></script>

    </body>
</html>
